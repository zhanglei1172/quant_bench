#!/usr/bin/env python3
"""
é‡åŒ–è¯„æµ‹è‡ªåŠ¨åŒ–æµç¨‹
æ”¯æŒé€šè¿‡ YAML é…ç½®æ–‡ä»¶è¿›è¡Œè½¬æ¢ã€é‡åŒ–å’Œè¯„æµ‹
"""

import os
import sys
import yaml
import argparse
import subprocess
from pathlib import Path
from datetime import datetime
from typing import List, Dict, Any
import json
import time
from threading import Lock
from loguru import logger

PROJECT_ROOT = Path(__file__).parent
PIPELINE_DIR = PROJECT_ROOT / "pipline"
LOGS_DIR = PROJECT_ROOT / "logs"
RESULTS_DIR = PROJECT_ROOT / "results"

# åˆ›å»ºå¸¦æ—¶é—´æˆ³çš„æ—¥å¿—ç›®å½•
timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
LOGS_DIR = LOGS_DIR / timestamp
LOGS_DIR.mkdir(parents=True, exist_ok=True)
RESULTS_DIR.mkdir(parents=True, exist_ok=True)

# é…ç½® loguru
logger.remove()  # ç§»é™¤é»˜è®¤ handler
logger.add(
    sys.stderr,
    format="<green>{time:YYYY-MM-DD HH:mm:ss}</green> | <level>{level: <8}</level> | <cyan>{name}</cyan>:<cyan>{function}</cyan>:<cyan>{line}</cyan> - <level>{message}</level>",
    level="INFO"
)

# æ·»åŠ  rich è¿›åº¦æ˜¾ç¤º
from rich.console import Console
from rich.table import Table
from rich.live import Live
from rich.text import Text
from enum import Enum

class TaskStatus(Enum):
    PENDING = "â³ ç­‰å¾…"
    RUNNING = "ğŸ”„ è¿›è¡Œä¸­"
    COMPLETED = "âœ… å®Œæˆ"
    SKIPPED = "â­ï¸ è·³è¿‡"
    FAILED = "âŒ å¤±è´¥"

class TaskTracker:
    """ä»»åŠ¡çŠ¶æ€è¿½è¸ªå™¨ï¼ˆçº¿ç¨‹å®‰å…¨ï¼‰"""
    def __init__(self):
        self.tasks = {}  # task_id -> {name, status, gpu, message}
        self.lock = Lock()
        self.console = Console()
        self.live = None
        self.start_time = time.time()
    
    def add_task(self, task_id: str, name: str, task_type: str):
        """æ·»åŠ æ–°ä»»åŠ¡"""
        with self.lock:
            self.tasks[task_id] = {
                'name': name,
                'type': task_type,
                'status': TaskStatus.PENDING,
                'gpu': None,
                'message': '',
                'start_time': None,
                'end_time': None,
            }
    
    def update_status(self, task_id: str, status: TaskStatus, gpu: List[int] = None, message: str = ''):
        """æ›´æ–°ä»»åŠ¡çŠ¶æ€"""
        with self.lock:
            if task_id is not None and task_id in self.tasks:
                self.tasks[task_id]['status'] = status
                if gpu is not None:
                    self.tasks[task_id]['gpu'] = gpu
                if message:
                    self.tasks[task_id]['message'] = message
                
                if status == TaskStatus.RUNNING and self.tasks[task_id]['start_time'] is None:
                    self.tasks[task_id]['start_time'] = time.time()
                elif status in [TaskStatus.COMPLETED, TaskStatus.SKIPPED, TaskStatus.FAILED]:
                    self.tasks[task_id]['end_time'] = time.time()
        
        self.refresh()
    
    def generate_table(self) -> Table:
        """ç”Ÿæˆè¿›åº¦è¡¨æ ¼"""
        table = Table(title="ğŸš€ é‡åŒ–è¯„æµ‹æµæ°´çº¿è¿›åº¦", show_header=True, header_style="bold magenta")
        table.add_column("ID", style="dim", width=4)
        table.add_column("ç±»å‹", width=8)
        table.add_column("ä»»åŠ¡åç§°", width=40)
        table.add_column("çŠ¶æ€", width=12)
        table.add_column("GPU", width=8)
        table.add_column("è€—æ—¶", width=10)
        table.add_column("å¤‡æ³¨", width=30)
        
        with self.lock:
            # æŒ‰ç±»å‹å’ŒIDæ’åº
            sorted_tasks = sorted(
                self.tasks.items(),
                key=lambda x: (
                    0 if x[1]['type'] == 'åŸºçº¿' else 1,
                    x[0]
                )
            )
            
            for task_id, info in sorted_tasks:
                # çŠ¶æ€æ ·å¼
                status = info['status']
                if status == TaskStatus.COMPLETED:
                    status_text = Text(status.value, style="bold green")
                elif status == TaskStatus.RUNNING:
                    status_text = Text(status.value, style="bold yellow")
                elif status == TaskStatus.FAILED:
                    status_text = Text(status.value, style="bold red")
                elif status == TaskStatus.SKIPPED:
                    status_text = Text(status.value, style="bold cyan")
                else:
                    status_text = Text(status.value, style="dim")
                
                # GPU æ˜¾ç¤º
                gpu_text = ",".join(map(str, info['gpu'])) if info['gpu'] else "-"
                
                # è€—æ—¶è®¡ç®—
                if info['start_time']:
                    if info['end_time']:
                        elapsed = info['end_time'] - info['start_time']
                    else:
                        elapsed = time.time() - info['start_time']
                    elapsed_text = f"{elapsed:.1f}s"
                else:
                    elapsed_text = "-"
                
                # ä»»åŠ¡åç§°æˆªæ–­
                name = info['name']
                if len(name) > 38:
                    name = name[:35] + "..."
                
                table.add_row(
                    task_id.split('-')[-1],  # åªæ˜¾ç¤ºæ•°å­—ID
                    info['type'],
                    name,
                    status_text,
                    gpu_text,
                    elapsed_text,
                    info['message'][:28] + "..." if len(info['message']) > 30 else info['message']
                )
            
            # æ·»åŠ ç»Ÿè®¡ä¿¡æ¯
            stats = self.get_statistics()
            total_time = time.time() - self.start_time
            
            table.caption = (
                f"æ€»ä»»åŠ¡: {stats['total']} | "
                f"å®Œæˆ: {stats['completed']} | "
                f"è·³è¿‡: {stats['skipped']} | "
                f"è¿›è¡Œä¸­: {stats['running']} | "
                f"ç­‰å¾…: {stats['pending']} | "
                f"å¤±è´¥: {stats['failed']} | "
                f"æ€»è€—æ—¶: {total_time:.1f}s"
            )
        
        return table
    
    def get_statistics(self) -> Dict[str, int]:
        """è·å–ç»Ÿè®¡ä¿¡æ¯"""
        stats = {
            'total': len(self.tasks),
            'pending': 0,
            'running': 0,
            'completed': 0,
            'skipped': 0,
            'failed': 0,
        }
        
        for info in self.tasks.values():
            status = info['status']
            if status == TaskStatus.PENDING:
                stats['pending'] += 1
            elif status == TaskStatus.RUNNING:
                stats['running'] += 1
            elif status == TaskStatus.COMPLETED:
                stats['completed'] += 1
            elif status == TaskStatus.SKIPPED:
                stats['skipped'] += 1
            elif status == TaskStatus.FAILED:
                stats['failed'] += 1
        
        return stats
    
    def start_live_display(self):
        """å¯åŠ¨å®æ—¶æ˜¾ç¤º"""
        self.live = Live(self.generate_table(), refresh_per_second=4, console=self.console)  # æé«˜åˆ·æ–°é¢‘ç‡
        self.live.start()
    
    def stop_live_display(self):
        """åœæ­¢å®æ—¶æ˜¾ç¤º"""
        if self.live:
            self.live.stop()
    
    def refresh(self):
        """åˆ·æ–°æ˜¾ç¤º"""
        if self.live:
            try:
                self.live.update(self.generate_table())
            except Exception:
                # å¿½ç•¥åˆ·æ–°è¿‡ç¨‹ä¸­çš„å¼‚å¸¸ï¼ˆä¾‹å¦‚ç»ˆç«¯å¤§å°å˜åŒ–ï¼‰
                pass

# å…¨å±€ä»»åŠ¡è¿½è¸ªå™¨
task_tracker = TaskTracker()

# å…¨å±€ç«¯å£åˆ†é…å™¨ï¼ˆçº¿ç¨‹å®‰å…¨ï¼‰
class PortAllocator:
    def __init__(self, start_port=8001):
        self.next_port = start_port
        self.lock = Lock()
    
    def allocate(self):
        with self.lock:
            port = self.next_port
            self.next_port += 1
            return port

port_allocator = PortAllocator()


# GPU åˆ†é…å™¨ï¼ˆçº¿ç¨‹å®‰å…¨ï¼‰
class GPUAllocator:
    def __init__(self, devices: List[int], pipeline_parallel_size: int = 1):
        self.devices = devices
        self.pipeline_parallel_size = pipeline_parallel_size
        self.lock = Lock()
        # è®¡ç®—å¯ç”¨çš„ GPU ç»„
        self.num_groups = len(devices) // pipeline_parallel_size
        if self.num_groups == 0:
            raise ValueError(f"GPU æ•°é‡ ({len(devices)}) ä¸è¶³ä»¥æ”¯æŒ pipeline_parallel_size={pipeline_parallel_size}")
        # è®°å½•æ¯ä¸ª GPU ç»„æ˜¯å¦è¢«å ç”¨
        self.group_in_use = [False] * self.num_groups
        self.condition = None  # å»¶è¿Ÿåˆå§‹åŒ–
    
    def _get_condition(self):
        """å»¶è¿Ÿåˆå§‹åŒ– Condition å¯¹è±¡"""
        if self.condition is None:
            from threading import Condition
            self.condition = Condition(self.lock)
        return self.condition
    
    def allocate(self, timeout: float = None) -> List[int]:
        """åˆ†é…ä¸€ç»„ GPUï¼Œå¦‚æœæ²¡æœ‰å¯ç”¨çš„åˆ™ç­‰å¾…"""
        import time
        condition = self._get_condition()
        
        start_time = time.time()
        with condition:
            while True:
                # æŸ¥æ‰¾ç©ºé—²çš„ GPU ç»„
                for group_idx in range(self.num_groups):
                    if not self.group_in_use[group_idx]:
                        self.group_in_use[group_idx] = True
                        start_gpu = group_idx * self.pipeline_parallel_size
                        gpus = self.devices[start_gpu:start_gpu + self.pipeline_parallel_size]
                        logger.debug(f"åˆ†é… GPU ç»„ {group_idx}: {gpus}")
                        return gpus
                
                # æ²¡æœ‰ç©ºé—²çš„ GPU ç»„ï¼Œç­‰å¾…
                if timeout is not None:
                    elapsed = time.time() - start_time
                    remaining = timeout - elapsed
                    if remaining <= 0:
                        raise TimeoutError("ç­‰å¾… GPU åˆ†é…è¶…æ—¶")
                    condition.wait(remaining)
                else:
                    condition.wait()
    
    def release(self, gpus: List[int]):
        """é‡Šæ”¾ GPU ç»„"""
        condition = self._get_condition()
        
        with condition:
            # æ ¹æ® GPU åˆ—è¡¨æ‰¾åˆ°å¯¹åº”çš„ç»„
            if len(gpus) > 0:
                first_gpu = gpus[0]
                try:
                    idx = self.devices.index(first_gpu)
                    group_idx = idx // self.pipeline_parallel_size
                    if 0 <= group_idx < self.num_groups:
                        self.group_in_use[group_idx] = False
                        logger.debug(f"é‡Šæ”¾ GPU ç»„ {group_idx}: {gpus}")
                        condition.notify_all()
                except ValueError:
                    pass

# å…¨å±€ GPU åˆ†é…å™¨ï¼ˆåœ¨ main ä¸­åˆå§‹åŒ–ï¼‰
gpu_allocator: GPUAllocator = None


def load_yaml_config(config_path: str) -> Dict[str, Any]:
    """åŠ è½½ YAML é…ç½®æ–‡ä»¶"""
    path = Path(config_path)
    with open(path, 'r', encoding='utf-8') as f:
        config = yaml.safe_load(f)

    # æ£€æŸ¥é…ç½®æ˜¯å¦ä¸ºç©ºï¼Œæ·»åŠ  dummy é”®é¿å…åç»­å¤„ç†å‡ºé”™
    if config is None:
        config = {'_empty': True}
    
    # ä»æ–‡ä»¶åæå– name å’Œ output_suffix
    # å¦‚æœé…ç½®æ–‡ä»¶ä¸­æ²¡æœ‰è¿™äº›å­—æ®µï¼Œåˆ™ä½¿ç”¨æ–‡ä»¶å
    filename = path.stem  # ä¸å«æ‰©å±•åçš„æ–‡ä»¶å
    if 'name' not in config:
        config['name'] = filename
    if 'output_suffix' not in config:
        config['output_suffix'] = f'-{filename}'
    
    # è§£æè·¯å¾„å­—æ®µï¼Œæ”¯æŒç›¸å¯¹è·¯å¾„
    if 'input_model' in config:
        config['input_model'] = resolve_path(config['input_model'], path.parent)
    
    return config


def resolve_path(path_str: str, base_dir: Path = None) -> str:
    """è§£æè·¯å¾„ï¼Œæ”¯æŒç›¸å¯¹è·¯å¾„å’Œç»å¯¹è·¯å¾„"""
    path = Path(path_str)
    if path.is_absolute():
        return str(path)
    else:
        # ç›¸å¯¹è·¯å¾„ç»Ÿä¸€ç›¸å¯¹äºé¡¹ç›®æ ¹ç›®å½•è§£æ
        return str((PROJECT_ROOT / path).resolve())


def get_configs_from_dir(directory: Path) -> List[Path]:
    """è·å–ç›®å½•ä¸‹æ‰€æœ‰ YAML é…ç½®æ–‡ä»¶"""
    return sorted(directory.glob("*.yaml"))


def run_transform(config: Dict[str, Any], base_model: str) -> str:
    """æ‰§è¡Œæ¨¡å‹è½¬æ¢ï¼ˆæœ¬è´¨ä¸Šæ˜¯ç¬¬ä¸€æ­¥çš„é‡åŒ–ï¼‰"""
    output_suffix = config.get('output_suffix')

    # å¦‚æœ output_suffix ä¸º Noneï¼Œç›´æ¥ä½¿ç”¨åŸºç¡€æ¨¡å‹è·¯å¾„ï¼ˆoriginal åŸºçº¿ï¼‰
    if output_suffix is None:
        logger.info(f"ä½¿ç”¨åŸå§‹æ¨¡å‹: {config['name']}")
        return base_model

    # æ‹¼æ¥è¾“å‡ºè·¯å¾„
    output_model = base_model + output_suffix

    # æ£€æŸ¥æ¨¡å‹æ˜¯å¦å·²å­˜åœ¨
    if Path(output_model).exists():
        logger.info(f"è·³è¿‡ {config['name']} (æ¨¡å‹å·²å­˜åœ¨)")
        return output_model

    logger.info(f"è½¬æ¢ {config['name']}")

    # å¤ç”¨é‡åŒ–è„šæœ¬ç”Ÿæˆé€»è¾‘
    quant_script = build_quant_script(base_model, output_model, config)

    # ä¿å­˜ä¸´æ—¶è„šæœ¬åˆ° logs ç›®å½•ï¼ˆä¸åˆ é™¤ï¼Œç”¨äº debugï¼‰
    model_name = Path(output_model).name
    tmp_script = LOGS_DIR / f"{model_name}_trans.py"
    with open(tmp_script, 'w') as f:
        f.write(quant_script)

    subprocess.run([sys.executable, str(tmp_script)], check=True, cwd=PROJECT_ROOT)
    
    # æˆåŠŸåå°†è„šæœ¬é‡å‘½åä¸ºéšè—æ–‡ä»¶
    hidden_script = LOGS_DIR / f".{model_name}_trans.py"
    tmp_script.rename(hidden_script)
    
    return output_model


def run_quantization(input_model: str, quant_config: Dict[str, Any], gpu_devices: List[int] = None, task_id: str = None) -> str:
    """æ‰§è¡Œæ¨¡å‹é‡åŒ–"""
    quant_name = quant_config['name']

    # æ„å»ºè¾“å‡ºè·¯å¾„
    quant_suffix = quant_config.get('output_suffix', '')
    output_model = input_model + quant_suffix
    model_name = Path(output_model).name

    # æ›´æ–°ä»»åŠ¡çŠ¶æ€
    task_tracker.update_status(task_id, TaskStatus.RUNNING, gpu_devices, "æ­£åœ¨é‡åŒ–...")

    # æ£€æŸ¥æ¨¡å‹æ˜¯å¦å·²å­˜åœ¨
    if Path(output_model).exists():
        logger.info(f"è·³è¿‡ {quant_name} (æ¨¡å‹å·²å­˜åœ¨)")
        task_tracker.update_status(task_id, TaskStatus.SKIPPED, message="æ¨¡å‹å·²å­˜åœ¨")
        return output_model

    logger.info(f"é‡åŒ– {quant_name} (GPU: {gpu_devices})")

    # æ„å»ºé‡åŒ–è„šæœ¬
    quant_script = build_quant_script(input_model, output_model, quant_config)

    # ä¿å­˜ä¸´æ—¶è„šæœ¬åˆ° logs ç›®å½•ï¼ˆä¸åˆ é™¤ï¼Œç”¨äº debugï¼‰
    tmp_script = LOGS_DIR / f"{model_name}_quant.py"
    with open(tmp_script, 'w') as f:
        f.write(quant_script)

    # æ—¥å¿—æ–‡ä»¶
    log_file = LOGS_DIR / f"{model_name}_llmcompressor.log"

    # è®¾ç½®ç¯å¢ƒå˜é‡ï¼šé™åˆ¶é‡åŒ–åªä½¿ç”¨æŒ‡å®šçš„GPU
    env = os.environ.copy()
    if gpu_devices:
        # å°†GPUåˆ—è¡¨è½¬æ¢ä¸ºé€—å·åˆ†éš”çš„å­—ç¬¦ä¸²
        env['CUDA_VISIBLE_DEVICES'] = ','.join(map(str, gpu_devices))

    with open(log_file, 'w') as log_f:
        result = subprocess.run([sys.executable, str(tmp_script)], check=False, cwd=PROJECT_ROOT, stdout=log_f, stderr=log_f, env=env)
    
    if result.returncode != 0:
        task_tracker.update_status(task_id, TaskStatus.FAILED, message="é‡åŒ–å¤±è´¥")
        raise RuntimeError(f"é‡åŒ–å¤±è´¥: {quant_name}")
    
    # æˆåŠŸåå°†è„šæœ¬å’Œæ—¥å¿—é‡å‘½åä¸ºéšè—æ–‡ä»¶
    hidden_script = LOGS_DIR / f".{model_name}_quant.py"
    hidden_log = LOGS_DIR / f".{model_name}_llmcompressor.log"
    tmp_script.rename(hidden_script)
    log_file.rename(hidden_log)
    
    return output_model


def build_quant_script(input_model: str, output_model: str, config: Dict[str, Any]) -> str:
    """æ„å»ºé‡åŒ–è„šæœ¬"""
    calib_cfg = config.get('calibration')
    quant_modifiers = config.get('quant_modifiers', {})
    save_compressed = config.get('save_compressed', False)

    # æ„å»º recipeï¼ˆå°† quant_modifiers åŒ…è£…åœ¨ recipe ç»“æ„ä¸­ï¼‰
    recipe = {'quant_stage': {'quant_modifiers': quant_modifiers}}
    recipe_yaml_str = yaml.dump(recipe, default_flow_style=False, sort_keys=False)

    # æ£€æŸ¥æ˜¯å¦æ˜¯ data-free é‡åŒ–
    is_data_free = calib_cfg is None

    # é»˜è®¤ save_compressed ä¸º False
    if save_compressed is None:
        save_compressed = False

    # ä»é…ç½®ä¸­è¯»å– dtypeï¼ˆé»˜è®¤ autoï¼‰
    dtype = config.get('dtype', 'auto')

    # æ˜¯å¦ä½¿ç”¨åŸå§‹ config.json æ›¿æ¢
    original_config_json = config.get('original_config_json', False)

    script = f"""
import torch
import yaml
import json
import os
import tempfile
import shutil
from transformers import AutoModelForCausalLM, AutoTokenizer
from llmcompressor import oneshot
{'from datasets import load_dataset' if not is_data_free else ''}

MODEL_ID = "{input_model}"
model = AutoModelForCausalLM.from_pretrained(MODEL_ID, dtype="{dtype}")
tokenizer = AutoTokenizer.from_pretrained(MODEL_ID)
"""

    if not is_data_free:
        script += f"""
# Load calibration dataset
ds = load_dataset("{calib_cfg['dataset_id']}", split="{calib_cfg['dataset_split']}")
ds = ds.shuffle(seed=42).select(range({calib_cfg['num_samples']}))

def preprocess(example):
    return {{"text": tokenizer.apply_chat_template(example["messages"], tokenize=False)}}
ds = ds.map(preprocess)

def tokenize(sample):
    return tokenizer(
        sample["text"],
        padding=False,
        max_length={calib_cfg['max_sequence_length']},
        truncation=True,
        add_special_tokens=False,
    )
ds = ds.map(tokenize, remove_columns=ds.column_names)

# Save recipe to temp file and pass file path
with tempfile.NamedTemporaryFile(mode='w', suffix='.yaml', delete=False) as f:
    recipe_path = f.name
    f.write('''{recipe_yaml_str}''')

try:
    oneshot(
        model=model,
        dataset=ds,
        recipe=recipe_path,
        max_seq_length={calib_cfg['max_sequence_length']},
        num_calibration_samples={calib_cfg['num_samples']},
        save_compressed={save_compressed},
        trust_remote_code_model=True,
    )
finally:
    os.unlink(recipe_path)
"""
    else:
        script += f"""
# Data-free quantization
# Save recipe to temp file and pass file path
with tempfile.NamedTemporaryFile(mode='w', suffix='.yaml', delete=False) as f:
    recipe_path = f.name
    f.write('''{recipe_yaml_str}''')

try:
    oneshot(
        model=model,
        recipe=recipe_path,
        pipeline="datafree",
        save_compressed={save_compressed},
    )
finally:
    os.unlink(recipe_path)
"""

    script += f"""
SAVE_DIR = "{output_model}"

# å½“ save_compressed=False æ—¶ï¼Œä½¿ç”¨æƒé‡ç§»æ¤æ–¹å¼ä¿å­˜ï¼ˆé¿å…é‡åŒ–ç›¸å…³é…ç½®æ±¡æŸ“ï¼‰
if {not save_compressed}:
    import gc
    print(">>> [1/3] Quantization complete, preparing weight transplant...")
    
    # ä»åŸå§‹è·¯å¾„åŠ è½½çº¯å‡€æ¨¡å‹ç»“æ„åˆ° CPUï¼Œä½¿ç”¨ç›¸åŒçš„ dtype
    clean_model = AutoModelForCausalLM.from_pretrained(
        "{input_model}",
        dtype="{dtype}",
        device_map="cpu",
    )
    
    print(">>> [2/3] Transplanting quantized weights to clean model...")
    
    # å¤åˆ¶æƒé‡ï¼ˆGPU é‡åŒ–æ¨¡å‹ -> CPU çº¯å‡€æ¨¡å‹ï¼‰
    quantized_state_dict = model.state_dict()
    clean_state_dict = clean_model.state_dict()
    
    for name in clean_state_dict.keys():
        if name in quantized_state_dict:
            try:
                if clean_state_dict[name].shape == quantized_state_dict[name].shape:
                    clean_state_dict[name].copy_(quantized_state_dict[name].to("cpu"))
            except Exception as e:
                print(f"Warning: Failed to copy {{name}}: {{e}}")
    
    # é‡Šæ”¾åŸæ¨¡å‹æ˜¾å­˜
    del model
    del quantized_state_dict
    gc.collect()
    
    print(">>> [3/3] Saving clean model...")
    model = clean_model
    model.save_pretrained(SAVE_DIR, safe_serialization=True)
else:
    model.save_pretrained(SAVE_DIR, save_compressed={save_compressed})

tokenizer.save_pretrained(SAVE_DIR)

# å½“ original_config_json=True æ—¶ï¼Œä½¿ç”¨åŸå§‹çš„ config.json
if {original_config_json}:
    import shutil
    original_config_path = os.path.join("{input_model}", "config.json")
    new_config_path = os.path.join(SAVE_DIR, "config.json")
    
    if os.path.exists(original_config_path):
        # å¦‚æœæ–°é…ç½®å·²å­˜åœ¨ï¼Œå…ˆå¤‡ä»½
        if os.path.exists(new_config_path):
            shutil.copy2(new_config_path, new_config_path + ".before_original")
        # å¤åˆ¶åŸå§‹é…ç½®
        shutil.copy2(original_config_path, new_config_path)
        print(f"Replaced config.json with original from {{original_config_path}}")

if True:
    import json
    config_path = os.path.join(SAVE_DIR, "config.json")
    
    if os.path.exists(config_path):
        with open(config_path, 'r') as f:
            config_data = json.load(f)
        
        # ä¿®æ”¹ quantization_config ä¸­çš„ num_bits
        if 'quantization_config' in config_data:
            quant_config = config_data['quantization_config']
            
            # éå†æ‰€æœ‰ config_groups
            if 'config_groups' in quant_config:
                for group_name, group_config in quant_config['config_groups'].items():
                    # åªä¿®æ”¹åŒ…å« input_activations çš„é…ç½®ç»„ï¼ˆæ¿€æ´»é‡åŒ–ç»„ï¼‰
                    if 'input_activations' in group_config and group_config['input_activations'] is not None:
                        if 'weights' in group_config and 'num_bits' in group_config['weights']:
                            old_bits = group_config['weights']['num_bits']
                            if old_bits == 4:
                                group_config['weights']['num_bits'] = 8
                                print(f"Modified {{group_name}} weights num_bits from 4 to 8 (W4A8 group)")
                    else:
                        print(f"Skipped {{group_name}} (weight-only quantization, no input_activations)")
            
            # ä¿å­˜ä¿®æ”¹åçš„é…ç½®
            with open(config_path, 'w') as f:
                json.dump(config_data, f, indent=2)
            print(f"Updated config.json for w4a8 compatibility (stored as int8, loaded as w8a8)")
"""

    return script


def prepare_dataset(eval_config: Dict[str, Any]) -> str:
    """å‡†å¤‡è¯„æµ‹æ•°æ®é›†"""
    from evalscope.collections import CollectionSchema, DatasetInfo, WeightedSampler
    from evalscope.utils.io_utils import dump_jsonl_data

    # ä»é…ç½®è·å–æ•°æ®é›†åç§°
    dataset_name = eval_config['dataset_name']
    dataset_path = PROJECT_ROOT / f"data/{dataset_name}.jsonl"

    # æ£€æŸ¥æ•°æ®é›†æ˜¯å¦å·²å­˜åœ¨
    if dataset_path.exists():
        logger.info(f"ä½¿ç”¨ç°æœ‰æ•°æ®é›†: {dataset_path.name}")
        return str(dataset_path)

    logger.info(f"ç”Ÿæˆæ•°æ®é›†: {dataset_name}")

    # æ„å»ºæ•°æ®é›†é…ç½®
    datasets = []
    for ds_cfg in eval_config['datasets']:
        datasets.append(DatasetInfo(
            name=ds_cfg['name'],
            weight=ds_cfg['weight'],
            task_type=ds_cfg['task_type'],
            tags=ds_cfg['tags'],
            args={'few_shot_num': ds_cfg['few_shot_num']}
        ))

    schema = CollectionSchema(
        name=eval_config['dataset_name'],
        datasets=[
            CollectionSchema(name='Mixed', weight=1, datasets=datasets)
        ]
    )

    mixed_data = WeightedSampler(schema).sample(eval_config['sampling']['total_samples'])

    # ä¿å­˜æ•°æ®é›†
    dump_jsonl_data(mixed_data, str(dataset_path))
    return str(dataset_path)


def find_available_port() -> int:
    """è·å–ä¸‹ä¸€ä¸ªå¯ç”¨ç«¯å£ï¼ˆçº¿ç¨‹å®‰å…¨ï¼‰"""
    return port_allocator.allocate()


def wait_for_service(port: int, timeout: int = 400) -> bool:
    """ç­‰å¾…æœåŠ¡å¯åŠ¨ï¼Œæ£€æŸ¥ç«¯å£æ˜¯å¦å¯è®¿é—®"""
    import socket
    import urllib.request
    import urllib.error

    start_time = time.time()
    while time.time() - start_time < timeout:
        try:
            # å°è¯•è¿æ¥ç«¯å£
            with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as s:
                s.settimeout(1)
                if s.connect_ex(('127.0.0.1', port)) == 0:
                    # ç«¯å£å¯è¿æ¥ï¼Œå°è¯•å‘é€ HTTP è¯·æ±‚
                    try:
                        urllib.request.urlopen(f"http://127.0.0.1:{port}/health", timeout=5)
                        return True
                    except (urllib.error.URLError, urllib.error.HTTPError):
                        # health ç«¯ç‚¹å¯èƒ½ä¸å­˜åœ¨ï¼Œä½†æœåŠ¡å·²ç»å¯åŠ¨
                        pass
                    return True
        except (socket.timeout, socket.error):
            pass
        time.sleep(2)
    return False


def run_evaluation(model_path: str, eval_config: Dict[str, Any], dataset_path: str, task_idx: int, total_tasks: int, gpu_devices: List[int] = None, task_id: str = None) -> Dict[str, float]:
    """æ‰§è¡Œè¯„æµ‹"""
    model_name = Path(model_path).name
    port = find_available_port()

    # è·å– GPU èµ„æºé…ç½®
    gpu_resources = eval_config.get('gpu_resources', {})
    pipeline_parallel_size = gpu_resources.get('pipeline_parallel_size', 1)

    # å¦‚æœæ²¡æœ‰æŒ‡å®š gpu_devicesï¼Œä½¿ç”¨å…¨å±€åˆ†é…å™¨åˆ†é…
    allocated_gpus = None
    if gpu_devices is None:
        if gpu_allocator is not None:
            gpu_devices = gpu_allocator.allocate()
            allocated_gpus = gpu_devices  # è®°å½•éœ€è¦é‡Šæ”¾çš„ GPU
        else:
            # é™çº§ï¼šä½¿ç”¨é»˜è®¤è®¾å¤‡
            devices = gpu_resources.get('devices', [0])
            gpu_devices = devices[:pipeline_parallel_size]

    # è·å–è¾“å‡ºç›®å½•é…ç½®
    work_dir = eval_config.get('output', {}).get('work_dir', 'outputs')
    dataset_name = eval_config['dataset_name']

    # å›ºå®šä½¿ç”¨ no_timestamp=Trueï¼Œè¾“å‡ºç›®å½•ä¸º eval_<config_name>
    output_dir = PROJECT_ROOT / work_dir

    # æ—¥å¿—æ–‡ä»¶
    vllm_log_file = LOGS_DIR / f"{model_name}_vllm.log"
    eval_log_file = LOGS_DIR / f"{model_name}_evalscope.log"

    # æ£€æŸ¥è¯„æµ‹ç»“æœæ˜¯å¦å·²å­˜åœ¨
    report_dir = PROJECT_ROOT / work_dir / "reports" / model_name
    report_path = report_dir / "data_collection.json"

    if report_path.exists():
        logger.info(f"è·³è¿‡ {model_name} (ç»“æœå·²å­˜åœ¨)")
        results = parse_evaluation_results(model_name, output_dir)
        # æ›´æ–°ä»»åŠ¡çŠ¶æ€
        score = results.get('overall_score', 0)
        task_tracker.update_status(task_id, TaskStatus.SKIPPED, message=f"åˆ†æ•°: {score}")
        # é‡Šæ”¾ GPU
        if allocated_gpus is not None and gpu_allocator is not None:
            gpu_allocator.release(allocated_gpus)
        return results

    # æ›´æ–°ä»»åŠ¡çŠ¶æ€
    task_tracker.update_status(task_id, TaskStatus.RUNNING, gpu_devices, "å¯åŠ¨vLLMæœåŠ¡...")

    logger.info(f"è¯„æµ‹ {model_name} (GPU:{gpu_devices})")

    # å¯åŠ¨ vLLM æœåŠ¡
    vllm_cmd = [
        "vllm", "serve", model_path,
        f"--port={port}",
        f"--pipeline-parallel-size={pipeline_parallel_size}"
    ]

    env = os.environ.copy()
    # å°†GPUåˆ—è¡¨è½¬æ¢ä¸ºé€—å·åˆ†éš”çš„å­—ç¬¦ä¸²
    env['CUDA_VISIBLE_DEVICES'] = ','.join(map(str, gpu_devices))

    # ä¿å­˜ vLLM å¯åŠ¨å‘½ä»¤åˆ° sh æ–‡ä»¶
    vllm_sh_file = LOGS_DIR / f"{model_name}_vllm.sh"
    with open(vllm_sh_file, 'w') as f:
        f.write(f"#!/bin/bash\n")
        f.write(f"export CUDA_VISIBLE_DEVICES={','.join(map(str, gpu_devices))}\n")
        f.write(f"{' '.join(vllm_cmd)}\n")

    # é‡å®šå‘ vLLM æ—¥å¿—åˆ°æ–‡ä»¶
    with open(vllm_log_file, 'w') as vllm_log:
        vllm_process = subprocess.Popen(vllm_cmd, env=env, stdout=vllm_log, stderr=vllm_log)

    # ç­‰å¾…æœåŠ¡çœŸæ­£å¯ç”¨
    logger.info(f"ç­‰å¾… vLLM æœåŠ¡å¯åŠ¨ (ç«¯å£: {port})...")
    task_tracker.update_status(task_id, TaskStatus.RUNNING, gpu_devices, "ç­‰å¾…vLLMå¯åŠ¨...")
    
    service_ready = wait_for_service(port)
    if not service_ready:
        logger.error(f"vLLM æœåŠ¡å¯åŠ¨è¶…æ—¶ (ç«¯å£: {port})")
        vllm_process.terminate()
        vllm_process.wait()
        task_tracker.update_status(task_id, TaskStatus.FAILED, message="vLLMå¯åŠ¨è¶…æ—¶")
        # é‡Šæ”¾ GPU
        if allocated_gpus is not None and gpu_allocator is not None:
            gpu_allocator.release(allocated_gpus)
        raise RuntimeError(f"vLLM æœåŠ¡å¯åŠ¨è¶…æ—¶ (ç«¯å£: {port})")
    logger.info(f"vLLM æœåŠ¡å¯åŠ¨æˆåŠŸ {model_name} (ç«¯å£: {port}ï¼ŒGPU: {gpu_devices})")
    
    task_tracker.update_status(task_id, TaskStatus.RUNNING, gpu_devices, "æ­£åœ¨è¯„æµ‹...")

    try:
        # è¿è¡Œè¯„æµ‹
        eval_script = build_eval_script(model_path, eval_config, dataset_path, port, str(output_dir))
        tmp_script = LOGS_DIR / f"{model_name}_eval.py"
        with open(tmp_script, 'w') as f:
            f.write(eval_script)

        # é‡å®šå‘ evalscope æ—¥å¿—åˆ°æ–‡ä»¶
        with open(eval_log_file, 'w') as eval_log:
            result = subprocess.run([sys.executable, str(tmp_script)], check=False, cwd=PROJECT_ROOT, stdout=eval_log, stderr=eval_log)

        if result.returncode != 0:
            task_tracker.update_status(task_id, TaskStatus.FAILED, message="è¯„æµ‹å¤±è´¥")
            raise RuntimeError(f"è¯„æµ‹å¤±è´¥: {model_name}")

        # è§£æç»“æœ
        results = parse_evaluation_results(model_name, output_dir)
        
        # æ›´æ–°ä»»åŠ¡çŠ¶æ€
        overall_score = results.get('overall_score', 0)
        task_tracker.update_status(task_id, TaskStatus.COMPLETED, message=f"åˆ†æ•°: {overall_score:.4f}")
        
        # æˆåŠŸåå°†æ‰€æœ‰ç›¸å…³æ–‡ä»¶é‡å‘½åä¸ºéšè—æ–‡ä»¶
        hidden_vllm_sh = LOGS_DIR / f".{model_name}_vllm.sh"
        hidden_vllm_log = LOGS_DIR / f".{model_name}_vllm.log"
        hidden_eval_script = LOGS_DIR / f".{model_name}_eval.py"
        hidden_eval_log = LOGS_DIR / f".{model_name}_evalscope.log"
        
        vllm_sh_file.rename(hidden_vllm_sh)
        vllm_log_file.rename(hidden_vllm_log)
        tmp_script.rename(hidden_eval_script)
        eval_log_file.rename(hidden_eval_log)
        
        return results

    except Exception as e:
        task_tracker.update_status(task_id, TaskStatus.FAILED, message=str(e)[:30])
        raise
    finally:
        # åœæ­¢ vLLM æœåŠ¡
        vllm_process.terminate()
        vllm_process.wait()
        # é‡Šæ”¾ GPU
        if allocated_gpus is not None and gpu_allocator is not None:
            gpu_allocator.release(allocated_gpus)


def build_eval_script(model_path: str, eval_config: Dict[str, Any], dataset_path: str, port: int, output_dir: str) -> str:
    """æ„å»ºè¯„æµ‹è„šæœ¬"""
    gen_cfg = eval_config['eval_config']['generation_config']

    script = f"""
from evalscope import TaskConfig, run_task
from evalscope.constants import EvalType
import os

task = TaskConfig(
    model="{model_path}",
    api_url="http://127.0.0.1:{port}/v1/chat/completions",
    api_key="EMPTY",
    eval_type=EvalType.SERVICE,
    datasets=['data_collection'],
    dataset_args={{
        'data_collection': {{
            'local_path': '{dataset_path}',
        }}
    }},
    eval_batch_size={eval_config['eval_config']['eval_batch_size']},
    generation_config={{
        'max_tokens': {gen_cfg['max_tokens']},
        'temperature': {gen_cfg['temperature']},
    }},
    no_timestamp=True,
    work_dir="{output_dir}",
)

run_task(task)
"""
    return script


def parse_evaluation_results(model_name: str, output_dir: Path = None) -> Dict[str, float]:
    """è§£æè¯„æµ‹ç»“æœ"""
    # åœ¨ outputs ç›®å½•ä¸­æŸ¥æ‰¾æœ€æ–°çš„ç»“æœ
    outputs_dir = PROJECT_ROOT / "outputs"
    if not outputs_dir.exists():
        return {}

    # å¦‚æœæŒ‡å®šäº†è¾“å‡ºç›®å½•ï¼Œç›´æ¥ä½¿ç”¨
    if output_dir:
        reports_dir = output_dir / "reports" / model_name
    else:
        # æ‰¾åˆ°æœ€æ–°çš„è¯„æµ‹ç›®å½•
        try:
            latest_dir = max(outputs_dir.iterdir(), key=lambda p: p.stat().st_mtime)
            reports_dir = latest_dir / "reports" / model_name
        except (ValueError, FileNotFoundError):
            return {}

    if not reports_dir.exists():
        return {}

    # è§£æç»“æœæ–‡ä»¶
    results = {}
    
    # 1. è¯»å–æ€»ä½“åˆ†æ•° (data_collection.json)
    data_collection_path = reports_dir / "data_collection.json"
    if data_collection_path.exists():
        with open(data_collection_path, 'r') as f:
            data = json.load(f)
            # æå–æ€»ä½“åˆ†æ•°
            overall_score = data.get('score')
            if overall_score is not None:
                results['overall_score'] = overall_score
    
    # 2. è¯»å–æ•°æ®é›†çº§åˆ«åˆ†æ•° (collection_detailed_report.json)
    report_path = reports_dir / "collection_detailed_report.json"
    if report_path.exists():
        with open(report_path, 'r') as f:
            report = json.load(f)
            # ä» dataset_level ä¸­æå–å„æ•°æ®é›†çš„ weighted_avg åˆ†æ•°
            dataset_level = report.get('dataset_level', [])
            for dataset in dataset_level:
                dataset_name = dataset.get('dataset_name', '')
                weighted_avg = dataset.get('weighted_avg.', 0.0)
                if dataset_name:
                    results[dataset_name] = weighted_avg

    return results


def save_summary_csv(all_results: List[Dict[str, Any]], output_path: str, dataset_name: str):
    """ä¿å­˜ç»“æœæ±‡æ€» CSVï¼ˆå¢é‡å†™å…¥ï¼Œæ–°ç»“æœæŒ‰ overall_score é™åºæ’åˆ—ï¼‰"""
    import csv

    if not all_results:
        print("[SUMMARY] æ²¡æœ‰ç»“æœéœ€è¦ä¿å­˜")
        return

    # æ·»åŠ æµ‹è¯•é›†åç§°å’Œæ—¶é—´æˆ³
    timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    
    # ä»ç»“æœä¸­åŠ¨æ€è·å–æ‰€æœ‰è¯„æµ‹æŒ‡æ ‡å­—æ®µ
    if all_results:
        # å›ºå®šå­—æ®µï¼ˆè°ƒæ•´é¡ºåºï¼Œå°† overall_score æ”¾åœ¨å‰é¢ï¼‰
        fieldnames = ['dataset', 'timestamp', 'transform', 'quantization', 'overall_score', 'model_path']
        # åŠ¨æ€æ·»åŠ è¯„æµ‹æŒ‡æ ‡å­—æ®µï¼ˆæ’é™¤å›ºå®šå­—æ®µï¼‰
        first_result = all_results[0]
        for key in first_result.keys():
            if key not in fieldnames:
                fieldnames.append(key)

    # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨ï¼Œç¡®å®šæ˜¯å¢é‡å†™å…¥è¿˜æ˜¯è¦†ç›–å†™å…¥
    file_exists = Path(output_path).exists()
    
    # å¦‚æœæ–‡ä»¶å­˜åœ¨ï¼Œè¯»å–ç°æœ‰æ•°æ®ç”¨äºæŸ¥æ‰¾é‡å¤é¡¹
    existing_rows = {}
    if file_exists:
        with open(output_path, 'r', encoding='utf-8') as f:
            reader = csv.DictReader(f)
            for row in reader:
                # ä½¿ç”¨ transform + quantization + dataset ä½œä¸ºå”¯ä¸€é”®
                key = (row['transform'], row['quantization'], row['dataset'])
                existing_rows[key] = row

    # æ–°ç»“æœæŒ‰ overall_score é™åºæ’åº
    def get_overall_score(result):
        try:
            return float(result.get('overall_score', 0))
        except (ValueError, TypeError):
            return 0.0
    
    all_results.sort(key=get_overall_score, reverse=True)

    # å†™å…¥æ•°æ®ï¼ˆè¿½åŠ æ¨¡å¼ï¼‰
    write_mode = 'a' if file_exists else 'w'
    with open(output_path, write_mode, newline='', encoding='utf-8') as f:
        writer = csv.DictWriter(f, fieldnames=fieldnames)
        
        # å¦‚æœæ˜¯æ–°å»ºæ–‡ä»¶ï¼Œå†™å…¥è¡¨å¤´
        if not file_exists:
            writer.writeheader()
        
        # å†™å…¥ç»“æœ
        for result in all_results:
            key = (result['transform'], result['quantization'], dataset_name)
            
            # æ·»åŠ é¢å¤–å­—æ®µ
            result['dataset'] = dataset_name
            result['timestamp'] = timestamp
            
            writer.writerow(result)
    
    logger.info(f"ç»“æœå·²ä¿å­˜åˆ°: {output_path} (æ–°å¢ {len(all_results)} æ¡è®°å½•)")


def build_result(transform_name: str, quant_name: str, model_path: str, eval_results: Dict[str, float]) -> Dict[str, Any]:
    """æ„å»ºç»“æœå­—å…¸ï¼ŒåŠ¨æ€åŒ…å«æ‰€æœ‰è¯„æµ‹å­—æ®µ"""
    result = {
        'transform': transform_name,
        'quantization': quant_name,
        'model_path': model_path,
    }
    # åŠ¨æ€æ·»åŠ æ‰€æœ‰è¯„æµ‹ç»“æœå­—æ®µ
    for key, value in eval_results.items():
        result[key] = value
    return result


def main():
    global gpu_allocator
    
    parser = argparse.ArgumentParser(description='é‡åŒ–è¯„æµ‹è‡ªåŠ¨åŒ–æµç¨‹')
    parser.add_argument('--trans-config', type=str, default=None, nargs='+',
                        help='è½¬æ¢é…ç½®æ–‡ä»¶è·¯å¾„ (æ”¯æŒå¤šä¸ªï¼Œé»˜è®¤ä½¿ç”¨ 1-trans ç›®å½•ä¸‹æ‰€æœ‰ YAML)')
    parser.add_argument('--quant-config', type=str, default=None, nargs='+',
                        help='é‡åŒ–é…ç½®æ–‡ä»¶è·¯å¾„ (æ”¯æŒå¤šä¸ªï¼Œé»˜è®¤ä½¿ç”¨ 2-quant ç›®å½•ä¸‹æ‰€æœ‰ YAML)')
    parser.add_argument('--eval-config', type=str, default=None,
                        help='è¯„æµ‹é…ç½®æ–‡ä»¶è·¯å¾„ (é»˜è®¤ä½¿ç”¨ 3-eval ç›®å½•ä¸‹çš„å•ä¸ª YAML)')
    parser.add_argument('--output-csv', type=str, default=None,
                        help='ç»“æœæ±‡æ€» CSV æ–‡ä»¶è·¯å¾„ (é»˜è®¤æ ¹æ®æ•°æ®é›†åç§°è‡ªåŠ¨ç”Ÿæˆ)')

    args = parser.parse_args()

    # åŠ è½½é…ç½®
    trans_configs = []
    quant_configs = []

    # åŠ è½½è¯„æµ‹é…ç½®
    if args.eval_config:
        eval_config = load_yaml_config(args.eval_config)
        logger.info(f"ä½¿ç”¨è¯„æµ‹é…ç½®: {args.eval_config}")
    else:
        eval_dir = PIPELINE_DIR / "3-eval"
        eval_files = list(eval_dir.glob("*.yaml"))
        if len(eval_files) == 0:
            raise RuntimeError(f"è¯„æµ‹é…ç½®ç›®å½•ä¸ºç©º: {eval_dir}")
        elif len(eval_files) > 1:
            raise RuntimeError(f"è¯„æµ‹é…ç½®ç›®å½•ä¸‹æœ‰å¤šä¸ª YAML æ–‡ä»¶: {[f.name for f in eval_files]}ï¼Œè¯·åªä¿ç•™ä¸€ä¸ª")
        else:
            eval_config = load_yaml_config(eval_files[0])
            logger.info(f"ä½¿ç”¨è¯„æµ‹é…ç½®: {eval_files[0].name}")

    # åˆå§‹åŒ–å…¨å±€ GPU åˆ†é…å™¨
    gpu_devices = eval_config['gpu_resources']['devices']
    pipeline_parallel_size = eval_config['gpu_resources']['pipeline_parallel_size']
    gpu_allocator = GPUAllocator(gpu_devices, pipeline_parallel_size)
    max_workers = gpu_allocator.num_groups
    
    logger.info(f"GPU é…ç½®: {len(gpu_devices)} ä¸ª GPU, PP={pipeline_parallel_size}, æœ€å¤§å¹¶å‘={max_workers}")

    # æ ¹æ® dataset_name ç”Ÿæˆ CSV æ–‡ä»¶å
    dataset_name = eval_config['dataset_name']
    if args.output_csv is None:
        args.output_csv = RESULTS_DIR / f"results_{dataset_name}.csv"
    else:
        args.output_csv = Path(args.output_csv)

    # åŠ è½½è½¬æ¢é…ç½®
    if args.trans_config:
        for cfg_path in args.trans_config:
            trans_configs.append(load_yaml_config(cfg_path))
    else:
        trans_dir = PIPELINE_DIR / "1-trans"
        for cfg_path in get_configs_from_dir(trans_dir):
            trans_configs.append(load_yaml_config(cfg_path))

    # åŠ è½½é‡åŒ–é…ç½®
    if args.quant_config:
        for cfg_path in args.quant_config:
            quant_configs.append(load_yaml_config(cfg_path))
    else:
        quant_dir = PIPELINE_DIR / "2-quant"
        for cfg_path in get_configs_from_dir(quant_dir):
            quant_configs.append(load_yaml_config(cfg_path))

    print(f"\n{'='*60}")
    print(f"é…ç½®: {len(trans_configs)} è½¬æ¢ Ã— {len(quant_configs)} é‡åŒ– = {len(trans_configs) * len(quant_configs)} ä»»åŠ¡")
    print(f"  ç»“æœ: {args.output_csv}")
    print(f"  æ—¥å¿—: {LOGS_DIR}")
    print(f"{'='*60}\n")

    # ä»è¯„æµ‹é…ç½®è·å–åŸºç¡€æ¨¡å‹è·¯å¾„
    if 'base_model' not in eval_config:
        raise RuntimeError(f"è¯„æµ‹é…ç½®ç¼ºå°‘ base_model å­—æ®µ")

    base_model = Path(eval_config['base_model'])
    if not base_model.exists():
        raise RuntimeError(f"åŸºç¡€æ¨¡å‹è·¯å¾„ä¸å­˜åœ¨: {base_model}")

    base_model_str = str(base_model)
    logger.info(f"åŸºç¡€æ¨¡å‹è·¯å¾„: {base_model}")

    # å­˜å‚¨æ‰€æœ‰ç»“æœ
    all_results = []

    # å‡†å¤‡è¯„æµ‹æ•°æ®é›†
    dataset_path = prepare_dataset(eval_config)

    # é˜¶æ®µ1: é¡ºåºæ‰§è¡Œæ‰€æœ‰è½¬æ¢
    print(f"\n[PHASE 1] æ‰§è¡Œè½¬æ¢ ({len(trans_configs)} ä¸ªé…ç½®)")

    trans_outputs = []
    for trans_cfg in trans_configs:
        logger.info(f"é˜¶æ®µ1 - è½¬æ¢: {trans_cfg['name']}")
        trans_output = run_transform(trans_cfg, base_model_str)
        trans_outputs.append((trans_cfg, trans_output))

    logger.info(f"é˜¶æ®µ1å®Œæˆ ({len(trans_outputs)} ä¸ªè½¬æ¢)")

    # é˜¶æ®µ2: å¹¶è¡Œæ‰§è¡Œé‡åŒ–å’Œè¯„æµ‹
    total_baseline = len(trans_outputs) + 1
    total_quant = len(trans_outputs) * len(quant_configs)
    total_tasks = total_baseline + total_quant
    logger.info(f"é˜¶æ®µ2 - å¹¶è¡Œå¯åŠ¨ {total_tasks} ä¸ªä»»åŠ¡ (åŸºçº¿: {total_baseline}, é‡åŒ–: {total_quant}, å¹¶å‘: {max_workers})")

    from concurrent.futures import ThreadPoolExecutor, as_completed

    # åˆå§‹åŒ–ä»»åŠ¡è¿½è¸ªå™¨
    task_idx = 0
    baseline_models = [('original', base_model_str)]
    baseline_models.extend([(cfg['name'], out) for cfg, out in trans_outputs])
    
    # æ·»åŠ åŸºçº¿ä»»åŠ¡
    for model_name, model_path in baseline_models:
        task_id = f"baseline-{task_idx}"
        task_tracker.add_task(task_id, model_name, "åŸºçº¿")
        task_idx += 1
    
    # æ·»åŠ é‡åŒ–ä»»åŠ¡
    for trans_cfg, trans_output in trans_outputs:
        for quant_cfg in quant_configs:
            task_id = f"quant-{task_idx}"
            task_name = f"{trans_cfg['name']}-{quant_cfg['name']}"
            task_tracker.add_task(task_id, task_name, "é‡åŒ–")
            task_idx += 1
    
    # å¯åŠ¨å®æ—¶è¿›åº¦æ˜¾ç¤º
    task_tracker.start_live_display()

    # æäº¤æ‰€æœ‰ä»»åŠ¡
    task_idx = 0
    futures = []
    with ThreadPoolExecutor(max_workers=max_workers) as executor:
        # ç¬¬ä¸€æ­¥ï¼šæäº¤åŸºçº¿æ¨¡å‹è¯„æµ‹ä»»åŠ¡
        for model_name, model_path in baseline_models:
            current_idx = task_idx
            task_id = f"baseline-{current_idx}"
            
            def eval_baseline(idx=current_idx, tid=task_id, name=model_name, path=model_path):
                try:
                    task_tracker.update_status(tid, TaskStatus.RUNNING, message="å‡†å¤‡è¯„æµ‹...")
                    # GPU ç”± run_evaluation å†…éƒ¨è‡ªåŠ¨åˆ†é…å’Œé‡Šæ”¾
                    results = run_evaluation(path, eval_config, dataset_path, idx, total_baseline, task_id=tid)
                    result = build_result(name, 'baseline', path, results)
                    logger.success(f"[åŸºçº¿ {idx + 1}/{total_baseline}] å®Œæˆ: {name}")
                    return result
                except Exception as e:
                    logger.error(f"[åŸºçº¿ {idx + 1}/{total_baseline}] å¤±è´¥: {name} - {e}")
                    task_tracker.update_status(tid, TaskStatus.FAILED, message=str(e)[:30])
                    return None
            futures.append(executor.submit(eval_baseline))
            task_idx += 1
        
        # ç¬¬äºŒæ­¥ï¼šæäº¤é‡åŒ–+è¯„æµ‹ä»»åŠ¡
        for trans_cfg, trans_output in trans_outputs:
            for quant_cfg in quant_configs:
                current_idx = task_idx
                task_id = f"quant-{current_idx}"
                trans_name = trans_cfg['name']
                quant_name = quant_cfg['name']
                
                def process_quant(idx=current_idx, tid=task_id, t_name=trans_name, q_name=quant_name, t_out=trans_output, q_cfg=quant_cfg):
                    gpus = None
                    try:
                        task_name = f"{t_name}-{q_name}"
                        
                        # åˆ†é… GPU
                        gpus = gpu_allocator.allocate()
                        task_tracker.update_status(tid, TaskStatus.RUNNING, gpus, "å‡†å¤‡é‡åŒ–...")

                        quant_output = run_quantization(t_out, q_cfg, gpus, task_id=tid)
                        task_tracker.update_status(tid, TaskStatus.RUNNING, gpus, "å‡†å¤‡è¯„æµ‹...")
                        # è¯„æµ‹æ—¶ä¼ å…¥å·²åˆ†é…çš„ GPUï¼Œé¿å…é‡å¤åˆ†é…
                        results = run_evaluation(quant_output, eval_config, dataset_path, idx, total_tasks, gpus, task_id=tid)

                        result = build_result(t_name, q_name, quant_output, results)
                        logger.success(f"[é‡åŒ– {idx - total_baseline + 1}/{total_quant}] å®Œæˆ: {task_name}")
                        return result
                    except Exception as e:
                        logger.error(f"[é‡åŒ– {idx - total_baseline + 1}/{total_quant}] å¤±è´¥: {t_name}-{q_name} - {e}")
                        task_tracker.update_status(tid, TaskStatus.FAILED, message=str(e)[:30])
                        return None
                    finally:
                        # ç¡®ä¿ GPU è¢«é‡Šæ”¾
                        if gpus is not None:
                            gpu_allocator.release(gpus)

                futures.append(executor.submit(process_quant))
                task_idx += 1

        # ç­‰å¾…ä»»åŠ¡å®Œæˆï¼Œæ”¶é›†ç»“æœ
        for future in as_completed(futures):
            result = future.result()
            if result:
                all_results.append(result)

        # åœæ­¢å®æ—¶æ˜¾ç¤º
        task_tracker.stop_live_display()
        
        # æ˜¾ç¤ºæœ€ç»ˆç»“æœè¡¨æ ¼
        print("\n" + "="*80)
        print("æœ€ç»ˆç»“æœ:")
        print("="*80)
        task_tracker.console.print(task_tracker.generate_table())

        # æ‰€æœ‰ä»»åŠ¡å®Œæˆåç»Ÿä¸€ä¿å­˜ç»“æœ
        dataset_name = eval_config['dataset_name']
        save_summary_csv(all_results, args.output_csv, dataset_name)

    logger.success(f"æ‰€æœ‰ä»»åŠ¡å®Œæˆ! ç»“æœ: {args.output_csv}")

if __name__ == '__main__':
    main()